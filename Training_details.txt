https://github.com/openai/CLIP/issues/150
1. 
- bfloat16
- train:  "./data/textaug_train_subset10k.json",
- val: "./data/textaug_valid_subset1k.json",
- "validation_split_percentage": 5,
-  "output_dir": "./tmp/model1",
-  "per_device_train_batch_size": 8,
    "per_device_eval_batch_size": 8,
    "gradient_accumulation_steps": 8,
-  "learning_rate": 5e-05,
    "weight_decay": 0.1,
    "adam_beta1": 0.9,
    "adam_beta2": 0.98,
    "adam_epsilon": 1e-06,
    "max_grad_norm": 1.0,
    "num_train_epochs": 4,

2. 
"dataset_name": null,
    "data_dir": "./data",
    "train_file": "./data/textaug_train_subset10k.json",
    "validation_file": "./data/textaug_valid_subset1k.json",
    "max_train_samples": null,
    "max_eval_samples": null,
    "overwrite_cache": false,
    "validation_split_percentage": 5,
    "preprocessing_num_workers": 2,
    "augment_images": true,
    "augment_captions": true,
    "captions_per_image": 5,
    "output_dir": "./tmp/model2",
    "overwrite_output_dir": false,
    "do_train": true,
    "do_eval": true,
    "evaluation_strategy": "steps",
    "per_device_train_batch_size": 8,
    "per_device_eval_batch_size": 8,
    "gradient_accumulation_steps": 8,
    "learning_rate": 5e-05,
    "weight_decay": 0.1,
    "adam_beta1": 0.9,
    "adam_beta2": 0.98,
    "adam_epsilon": 1e-06,
    "max_grad_norm": 1.0,
    "num_train_epochs": 4,
    "max_steps": -1,
    "lr_scheduler_type": "linear",
    "warmup_ratio": 0.2,
    "logging_first_step": false,
    "logging_steps": 50,
    "save_strategy": "epoch",
    "seed": 42,
    "dataloader_drop_last": true,
    "eval_steps": 200,
    "run_name": "test0-10k_float32_batch8",
    "adafactor": false,
    "report_to": "all",
    "skip_memory_metrics": true,
    "resume_from_checkpoint": null
}

3.    "model_name_or_path": "openai/clip-vit-base-patch32",
"dtype": "bfloat16",
    "save_optimizer": false,
    "dataset_name": null,
    "data_dir": "./data",
    "train_file": "./data/textaug_train_subset10k.json",
    "validation_file": "./data/textaug_valid_subset1k.json",
    "max_train_samples": null,
    "max_eval_samples": null,
    "overwrite_cache": false,
    "validation_split_percentage": 5,
    "preprocessing_num_workers": 2,
    "augment_images": true,
    "augment_captions": true,
    "captions_per_image": 5,
    "output_dir":  "./tmp/model3",
    "overwrite_output_dir": false,
    "do_train": true,
    "do_eval": true,
    "evaluation_strategy": "steps",
    "per_device_train_batch_size": 16,
    "per_device_eval_batch_size": 16,
    "gradient_accumulation_steps": 8,
    "learning_rate": 1e-7,
    "weight_decay": 0.001,
    "adam_beta1": 0.9,
    "adam_beta2": 0.98,
    "adam_epsilon": 1e-06,
    "max_grad_norm": 1.0,
    "num_train_epochs": 4,
    "max_steps": -1,
    "lr_scheduler_type": "linear",
    "warmup_ratio": 0.2,
    "logging_first_step": false,
    "logging_steps": 50,
    "save_strategy": "epoch",
    "seed": 42,
    "dataloader_drop_last": true,
    "eval_steps": 200,
    "run_name": "test3",
    "adafactor": false,
    "report_to": "all",
    "skip_memory_metrics": true,
    "resume_from_checkpoint": null
4. "model_name_or_path": "openai/clip-vit-base-patch32",
    "dtype": "bfloat16",
    "save_optimizer": false,
    "dataset_name": null,
    "data_dir": "./data",
    "train_file": "./data/textaug_train_subset10k.json",
    "validation_file": "./data/textaug_valid_subset1k.json",
    "max_train_samples": null,
    "max_eval_samples": null,
    "overwrite_cache": false,
    "validation_split_percentage": 5,
    "preprocessing_num_workers": 2,
    "augment_images": true,
    "augment_captions": true,
    "captions_per_image": 5,
    "output_dir": "./tmp/model4",
    "overwrite_output_dir": false,
    "do_train": true,
    "do_eval": true,
    "evaluation_strategy": "steps",
    "per_device_train_batch_size": 16,
    "per_device_eval_batch_size": 16,
    "gradient_accumulation_steps": 8,
    "learning_rate": 1e-8,
    "weight_decay": 0.001,
    "adam_beta1": 0.9,
    "adam_beta2": 0.98,
    "adam_epsilon": 1e-06,
    "max_grad_norm": 1.0,
    "num_train_epochs": 4,
    "max_steps": -1,
    "lr_scheduler_type": "linear",
    "warmup_ratio": 0.2,
    "logging_first_step": false,
    "logging_steps": 50,
    "save_strategy": "epoch",
    "seed": 42,
    "dataloader_drop_last": true,
    "eval_steps": 200,
    "run_name": "test4",
    "adafactor": false,
    "report_to": "all",
    "skip_memory_metrics": true,

5.     
 "model_name_or_path": "openai/clip-vit-base-patch32",
    "dtype": "bfloat16",
    "save_optimizer": false,
    "dataset_name": null,
    "data_dir": "./data/full_data",
    "train_file": "./data/full_data/textaug_train_full.json",
    "validation_file": "./data/full_data/textaug_valid_subset10k.json",
    "max_train_samples": null,
    "max_eval_samples": null,
    "overwrite_cache": false,
    "validation_split_percentage": 5,
    "preprocessing_num_workers": 2,
    "augment_images": true,
    "augment_captions": true,
    "captions_per_image": 5,
    "output_dir": "./tmp/model5",
    "overwrite_output_dir": false,
    "do_train": true,
    "do_eval": true,
    "evaluation_strategy": "steps",
    "per_device_train_batch_size": 16,
    "per_device_eval_batch_size": 16,
    "gradient_accumulation_steps": 8,
    "learning_rate": 1e-8,
    "weight_decay": 0.001,
    "adam_beta1": 0.9,
    "adam_beta2": 0.98,
    "adam_epsilon": 1e-06,
    "max_grad_norm": 1.0,
    "num_train_epochs": 4,
    "max_steps": -1,
    "lr_scheduler_type": "linear",
    "warmup_ratio": 0.2,
    "logging_first_step": false,
    "logging_steps": 50,
    "save_strategy": "epoch",
    "seed": 42,
    "dataloader_drop_last": true,
    "eval_steps": 200,
    "run_name": "test5",
    "adafactor": false,
    "report_to": "all",
    "skip_memory_metrics": true,
    "resume_from_checkpoint": null

6. 