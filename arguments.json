{
    "model_name_or_path": "openai/clip-vit-base-patch32",
    "dtype": "float32",
    "save_optimizer": false,
    "dataset_name": "my_dataset",
    "data_dir": "./data",
    "train_file": "/path/to/train.json",
    "validation_file": "/path/to/valid.json",
    "max_train_samples": 1000,
    "max_eval_samples": 100,
    "overwrite_cache": false,
    "validation_split_percentage": 10,
    "preprocessing_num_workers": 2,
    "augment_images": true,
    "augment_captions": false,
    "captions_per_image": 1,
    "output_dir": "./tmp/model1",
    "overwrite_output_dir": false,
    "do_train": true,
    "do_eval": true,
    "evaluation_strategy": "steps",
    "per_device_train_batch_size": 32,
    "per_device_eval_batch_size": 32,
    "gradient_accumulation_steps": 8,
    "learning_rate": 5e-05,
    "weight_decay": 0.1,
    "adam_beta1": 0.9,
    "adam_beta2": 0.98,
    "adam_epsilon": 1e-06,
    "max_grad_norm": 1.0,
    "num_train_epochs": 1,
    "max_steps": -1,
    "lr_scheduler_type": "linear",
    "warmup_ratio": 0.2,
    "logging_first_step": false,
    "logging_steps": 50,
    "save_strategy": "epoch",
    "seed": 42,
    "dataloader_drop_last": true,
    "eval_steps": 200,
    "run_name": null,
    "adafactor": false,
    "report_to": "all",
    "skip_memory_metrics": true,
    "resume_from_checkpoint": null
}
